[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n¿Cómo funciona ChatGPT? - Parte I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo utilizar la IA para entender mejor a sus usuarios?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué hablo de Inteligencia Artificial en español?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué es la inteligencia artificial? – de cero\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/10-05-2025/10-05-2025.html",
    "href": "blog/posts/10-05-2025/10-05-2025.html",
    "title": "¿Por qué hablo de Inteligencia Artificial en español?",
    "section": "",
    "text": "En 2025 creo que no hay persona que no haya escuchado acerca de la Inteligencia Artificial. Pero al preguntar qué es exactamente, para qué sirve o cómo funciona, la mayoría no sabe responder – especialmente en LATAM. En mi opinión, esto se debe principalmente a que no hay información sobre el tema en español y más específicamente se debe a tres factores : la cultura poco “tech”, la poca exposición a la IA y la falta de contenido técnico sobre el tema. Creé este blog con la intención de contribuir a que esto cambie.\nA medida que esta tecnología se va desarrollando se vuelve cada vez más relevante entenderla. Entenderla, por una parte, nos ayuda distinguir los riesgos infundados (como el rumor de que se está convirtiendo una entidad fuera de control) de los riesgos reales (como el hecho que puede amplificar ciertos sesgos que ya tenemos). Por otra parte, comprenderla nos permite identificar oportunidades de impacto positivo : desde cómo potenciar el crecimiento de negocios locales hasta implementar educación personalizada a gran escala. Finalmente, entenderla nos da la oportunidad de participar en la discusión. En lugar de delegar la decisión a un número limitado de personas acerca de cómo queremos que esta tecnología sea implementada, nos permite que sea una decisión colectiva."
  },
  {
    "objectID": "blog/posts/10-05-2025/10-05-2025.html#una-cultura-poco-tech",
    "href": "blog/posts/10-05-2025/10-05-2025.html#una-cultura-poco-tech",
    "title": "¿Por qué hablo de Inteligencia Artificial en español?",
    "section": "Una cultura poco “tech”",
    "text": "Una cultura poco “tech”\nMe encanta LATAM. Los memes de “no entenderían la vibra” o “imagínate vivir en Suiza y perderte esto” creo que ilustran perfectamente algunas de las cosas que hacen única nuestra región. Siempre me ha parecido curioso cómo el hablar español nos hace tener tantas cosas en común. Lastimosamente, a pesar de nuestra bella lengua, memes, comida, stickers y tradiciones, nos falta mucho a nivel de cultura tecnológica. A diferencia de otras regiones, los medios de comunicación no le dan el mismo peso a los avances tecnológicos – especialmente en IA. TechCrunch, The Verge, MKBHD, entre otros, son medios de comunicación estadounidenses que cubren exclusivamente temas de tecnología. En LATAM no tenemos un equivalente en mi opinión. Esto crea un círculo vicioso en el que nadie está informado, por lo tanto nadie le da importancia, entonces las pocas noticias que vemos en los medios no parecen ser importantes, entonces no se venden y por eso los medios deciden darle aún menos cobertura. Obvio, es fácil argumentar que tenemos problemas más serios que estar al día con el nuevo modelo de computadora que Apple presentó, pero muchas de estas noticias si son importantes para nosotros. Especialmente ahora que salen nuevos modelos de IA cada semana, estar al día con esta información nos permite identificar nuevos casos de uso ya que a medida que estos modelos van mejorando, nuevas posibilidades se van perfilando. Entre otros, a nivel individual esto nos permite identificar que partes de nuestras labores diarias se van automatizando (y que por lo tanto van siendo menos relevantes en el ámbito laboral) y a nivel de organizaciones esto permite identificar casos de uso que antes eran imposibles. No me parece exagerado decir que los avances se están dando en una escala de no meses, sino semanas. Por ejemplo, la posibilidad de editar o generar imágenes con texto ha mejorado significativamente recientemente. En tan solo un mes Google, Rêve, y OpenAI anunciaron modelos de este tipo. Cada uno – a mi parecer – superando el modelo anterior. Aquí están algunos ejemplos de imágenes generadas con IA a partir de una descripción textual que yo le dí al modelo.\n  \nAl no estar atentos a los avances en IA, se nos hace difícil ver las posibilidades que se van creando."
  },
  {
    "objectID": "blog/posts/10-05-2025/10-05-2025.html#mucho-ring-ring-y-muy-poco-helado",
    "href": "blog/posts/10-05-2025/10-05-2025.html#mucho-ring-ring-y-muy-poco-helado",
    "title": "¿Por qué hablo de Inteligencia Artificial en español?",
    "section": "Mucho ring ring y muy poco helado",
    "text": "Mucho ring ring y muy poco helado\nLos sistemas de recomendación son el tipo de IA con el cual estamos más familiarizados en LATAM. Cada vez más Youtube, Netflix y especialmente Instagram y Tiktok proponen el contenido exacto que buscamos antes de que lo busquemos, hasta el punto que a veces creemos que estas empresas escuchan nuestras conversaciones. (La cantidad de veces que he visto anuncios de un producto después de haber hablado de él es perturbante). Otro ejemplo que creo que mucha gente se ha cruzado son los asistentes de servicio al cliente – aunque la mayoría de veces dejan mucho que desear. Pero allí nos quedamos… estos son los casos típicos de los cuales hemos escuchado en LATAM. Para nosotros eso es todo lo que la IA puede hacer, porque es lo único que conocemos. A esto me refiero con “mucho ring ring y muy poco helado” (que sale de un sticker de WhatsApp). Muchos dicen que la IA promete cambiar todo… pero ¿cómo? ¿Se supone que un bot de servicio al cliente va a cambiar industrias enteras?\nHemos tenido muy poca exposición a otras aplicaciones menos conocidas de la IA, y algunas veces porque ni sabemos que por detrás hay IA (como los asistentes como Siri o autocorrectores en nuestros teléfonos). La exposición hace que uno se familiarice con la tecnología, y probándola es como uno ve el alcance que tiene. Mi ejemplo favorito de esto es la primera vez que le mostré ChatGPT a mis papás. Mi papá me dijo que le preguntara “¿Cuál es el río más caudaloso del mundo?”. Me sentí un poco decepcionado porque era una pregunta que Google puede responder, pero es simplemente que ellos no veían en ese momento que ChatGPT podía responder a preguntas que eran impensables hace un par años. Justo después le pedí a ChatGPT que me hiciera un ensayo acerca de la libertad – algo que Google no lograría, y algo que uno no se esperaría que una computadora pueda responder. Pero lo hizo. Probar e interactuar con la tecnología juega un rol crucial para entender de qué es capaz, y poder realmente sacarle provecho. No menos importante, esta misma interacción nos permite también identificar sus debilidades. Cada vez más escucho personas decir “ChatGPT me dijo [una cifra/hecho]”. Pero ChatGPT (y similares) no siempre proveen información factual, y hay muchísimos equipos trabajando en ese problema hoy en día porque aún no está resuelto. (De hecho este es un verdadero riesgo : cómo gestionar toda la desinformación o “fake news” que se pueden generar con estos modelos). Estar al tanto de esta debilidad crítica evita que creamos a ciegas en todo lo que el modelo dice.\nLas pocas interacciones que tenemos con la IA en LATAM nos pintan una imagen extremadamente limitada de lo que la IA es, de su verdadero alcance y de los riesgos más tangibles."
  },
  {
    "objectID": "blog/posts/10-05-2025/10-05-2025.html#educación-técnica",
    "href": "blog/posts/10-05-2025/10-05-2025.html#educación-técnica",
    "title": "¿Por qué hablo de Inteligencia Artificial en español?",
    "section": "Educación técnica",
    "text": "Educación técnica\nSi tomamos en cuenta que tenemos sistemas de educación muy deficientes, el hecho que no tenemos un nivel de inglés muy alto, y que 99.99% de los recursos de calidad están escritos en inglés, esto pone una barrera gigantesca para cualquiera que quiera aprender cómo funciona la IA, y es el caso en muchas otras disciplinas también. En mi caso, cuando estaba en el colegio, compré unos libros que – sin exageración alguna – cambiaron mi trayectoria académica. Pero esos libros no estaban en español, tampoco la mayoría de videos que vi en Youtube. Más tarde, cuando aprendí a programar, los recursos que vi tampoco estaban en español.\nVarios años después, la situación no es muy diferente. Casi todos los blogs de empresas que publican implementaciones de IA son de empresas estadounidenses y están escritos en inglés. Todos los artículos publicados acerca de avances técnicos se publican en inglés. Programas como DeepLearning.ai se desarrollaron en inglés. Aunque, justamente con la IA, estos cursos a veces proponen versiones traducidas al español, no es igual."
  },
  {
    "objectID": "blog/posts/22-10-2025/22-10-2025.html",
    "href": "blog/posts/22-10-2025/22-10-2025.html",
    "title": "¿Cómo funciona ChatGPT? - Parte I",
    "section": "",
    "text": "El objetivo de esta serie de blogs es explicar de manera simple cómo funciona ChatGPT. Quiero que cualquier persona se pueda hacer una idea de cómo funciona, incluso sin haber hecho matemáticas, informática, etc.\nAntes de empezar quiero dejar claro algo: ChatGPT es un producto y no una tecnología. La tecnología detrás de ChatGPT se llama “LLM” (para Large Language Models en inglés). De la misma manera que existen otros motores de búsqueda que no son Google como Yahoo, Bing, etc., existen otros LLMs que no son ChatGPT. A diferencia de los motores de búsqueda, cada variante de LLM tiene sus ventajas y desventajas. La mayoría de personas usa Google por puro hábito, pero objetivamente Google no es muy diferente de los otros motores de búsqueda. Este no es el caso de los LLMs : algunos LLMs son mucho mejores programando, otros son mucho mejores en tareas creativas, otros son mucho mejores para analizar imágenes, etc. De ahora en adelante hablaré de LLMs para referirme a cualquier sistema que funciona como ChatGPT para hacer énfasis en que ChatGPT no es siempre la solución y que según el caso hay otras variantes de LLMs que funcionarán mejor.\nComo lo indica su nombre, un modelo extenso de lenguaje (o LLM en inglés) es una manera de modelizar el lenguaje que nosotros, los humanos, utilizamos. En una frase, un LLM es un modelo que aprende a predecir la palabra siguiente. Todos hemos visto las sugerencias encima de los teclados de nuestros teléfonos. Cuando escribimos “Hol” el teléfono nos sugiere “Hola” porque es lo más probable que escribamos después de “Hol”. Un LLM hace lo mismo pero a una escala más grande. En lugar de predecir una sola letra como “a” al final de “Hola”, los LLMs pueden predecir palabras enteras basándose en oraciones enteras.\nA pesar de parecer una manera simplista de representar nuestro lenguaje, esta manera de abordar el problema de modelización ha sido extremadamente exitosa. Una de las razones está en la primera “L” de LLM que corresponde a “Large” es decir extenso o grande. Estos modelos son extensos/grandes en varios aspectos:\n\nTamaño: estos modelos pueden llegar a pesar más de 500GB. Para utilizarlos no bastan computadoras “normales” ya que estas no tienen la potencia de cálculo ni el espacio necesario.\n\nTiempo de desarrollo: durante varias semanas, miles de computadoras en “data centers” corren día y noche para encontrar los billones de parámetros que mejoran la calidad de los resultados producidos por el modelo.\nContexto: mencionamos que estos modelos predicen una palabra siguiente en función de las palabras anteriores. Algunos modelos pueden llegar a usar alrededor de 200 mil palabras para predecir la palabra que sigue.\n\nSi tuviéramos que adivinar lo que sigue después de “Hola”, algunas opciones son\n\n“Hola” si otra persona responde\n“!” si queremos expresar emoción\n“Qué” si queremos preguntar qué tal\n\nSi tuviéramos que adivinar lo que sigue después de “Hola, ¿cómo”, algunas opciones son\n\n“Hola, ¿cómo estás?”\n“Hola, ¿cómo vas?”\n“Hola, ¿cómo estuvo …”\n\nSi tuviéramos que adivinar lo que sigue después de “Hola, ¿cómo estuvo”, algunas opciones son\n\n“Hola, ¿cómo estuvo el almuerzo?”\n“Hola, ¿cómo estuvo la fiesta?”\netc.\n\nPor lo general, a medida que aumentamos el tamaño del contexto el número de palabras posibles se va reduciendo, y predecir la palabra siguiente se vuelve más fácil. Teniendo 200 mil palabras de contexto es concebible que estos modelos generen texto similar al que un humano produce.\nDurante muchos años, las tareas de procesamiento de lenguaje eran tratados como problemas separados. Es decir que cada tarea se intentaba resolver con un modelo específico. Por ejemplo, había modelos capaces de hacer traducción pero incapaces de resumir textos, los modelos capaces de entender el tono de un texto (positivo o negativo por ejemplo) no eran capaces de responder preguntas, etc. Los LLMs permitieron tener un modelo único que sirva para una gran variedad de tareas simultáneamente. Por esta razón se habla de “foundational models” o modelos fundacionales, porque son modelos que sirven de base para resolver varios problemas simultáneamente.\nEn resumen, los LLMs son modelos que aprenden a predecir la palabra más probable en función de palabras previas. A pesar de ser una modelización simple, la escala es el factor crucial que permite obtener resultados de tan buena calidad. Cómo crear un modelo que sea bueno prediciendo la palabra siguiente es el tema más técnico, pero en resumen sigue el mismo patrón que expliqué aquí : tomamos una serie de textos, predecimos la palabra siguiente para cada texto y comparamos con la palabra real que seguía, luego modificamos los parámetros del modelo para maximizar el número las respuestas correctas y repetimos. Haré un blog explicando los detalles técnicos de esto más adelante, pero por el momento esta idea general es suficiente.\nHasta ahora solo tenemos un modelo que predice la palabra siguiente, no una oración entera. Para generar oraciones enteras el procedimiento es bastante simple: Un usuario ingresa una oración inicial como “¿Cuál es la capital de Francia?”. Luego, el LLM predice la palabra siguiente más probable como “La”. Después, tomamos la oración original, le agregamos la predicción “La” y volvemos a pasarla por el LLM para predecir otra palabra. Repetimos este proceso y progresivamente obtenemos una oración que responde a nuestra pregunta.\nEtapa 1:\n\nEntrada LLM: “¿Cuál es la capital de Francia?”\nSalida LLM: “La”\n\nEtapa 2:\n\nEntrada LLM: “¿Cuál es la capital de Francia? La”\nSalida LLM: “capital”\n\nEtapa 3:\n\nEntrada LLM: “¿Cuál es la capital de Francia? La capital”\nSalida LLM: “de”\n\nEtapa 4:\n\nEntrada: “¿Cuál es la capital de Francia? La capital de”\nSalida: “Francia”\n\nEtapa 5:\n\nEntrada: “¿Cuál es la capital de Francia? La capital de Francia”\nSalida: “es”\n\nEtapa 6:\n\nEntrada: “¿Cuál es la capital de Francia? La capital de Francia es”\nSalida: “París”\n\nEspero que este ejemplo ilustre que no hay ningún tipo de razonamiento o consciencia que entra en juego a la hora de generar una respuesta.\nAl entender este mecanismo podemos explicar ciertos comportamientos de los LLMs. Primero, que un LLM diga algo que es falso pretendiendo que es verdadero es bastante común. Esto ocurre porque para responder a una pregunta el LLM genera la secuencia de palabras más probable y esto no garantiza que la respuesta sea correcta. Por ejemplo, si le preguntamos quién fue el primer alcalde de Santa Tecla, el LLM puede generar “José Martínez (1987-1991)” simplemente porque José es un nombre frecuente y Martínez es un apellido común también, y eso basta para que sea una respuesta probable. Segundo, esto también explica porqué los LLMs a veces fallan en razonamientos simples de matemática. Para hacer una multiplicación, los LLMs no realizan las etapas que aprendimos en primaria (que asegura que el resultado sea correcto), sino que predicen el resultado. Estos comportamientos se hacen menos frecuentes con la llegada del uso de herramientas como el acceso a una calculadora o a internet y nuevos métodos de entrenamiento como Reinforcement Learning. Sin embargo, el mecanismo sigue siendo el mismo. Incluso en el modo “razonamiento” se sigue prediciendo una palabra a la vez. Los LLMs son extremadamente útiles pero no tenemos que olvidar que por detrás no hay nada más que la predicción de la palabra más probable.\nPara crear algo similar a ChatGPT es importante que las respuestas sean similares a las de una conversación y que responda como un “asistente”. Nos gustaría tener respuestas más cercanas a “Necesitas que busque más información acerca de X?”, “Dime si tienes más dudas acerca de X!”, etc. en lugar de parrafos que se parezcan a lo que podríamos ver en Wikipiedia por ejemplo. En el próximo blog explicaré cómo lograr este estilo conversacional y para eso explicaré qué son los “tokens” y aclararé algunas simplificaciones que hice.\nSixto :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SC",
    "section": "",
    "text": "Hola, soy Sixto\n\n\nTrabajo en Inteligencia Artificial y en sus aplicaciones en empresas\n\n\n\n        \n\nAcerca de mí\n\n  \n  Me llamo Sixto Cerna, soy de El Salvador y estoy realizando mis estudios en París. He trabajado en startups y en laboratorios de Inteligencia Artificial (IA) en LATAM y Francia. \n  Creo que hay muy poca información en español acerca de la IA. Mi objetivo es compartir proyectos que 1) expliquen cómo funciona y 2) contribuyan a que más personas sean capaces de identificar oportunidades con esta tecnología. \n  Puedes leer mis artículos  aquí .\n  \n\nEducación\n\n\n  \n    \n      \n    \n    \n       Université PSL - Lycée Henri IV\n      Septiembre 2020 - Junio 2023\n      Paris\n    \n    \n      Ciclo Pluridisciplinario de Estudios Superiores (CPES)\n      Matemáticas e Informática\n    \n  \n\n  \n\n  \n    \n      \n    \n    \n      Université Paris Cité\n      Septiembre 2024 - Junio 2026\n      Paris\n    \n    \n      Maestría en Matemáticas e Informática\n      Inteligencia Articifial\n    \n  \n\n\n\nExperiencia profesional\n\n  \n    \n      \n    \n    \n      Kalto\n      Septiembre 2023 - Marzo 2024\n      Mexico (Remoto)\n    \n    \n      Científico de Datos\n    \n  \n\n  \n\n  \n    \n      \n    \n    \n      Tripartie\n      Julio 2022 - Agosto 2022\n      Paris\n    \n    \n      Pasante en Inteligencia Artificial\n    \n  \n\n  \n\n  \n    \n      \n    \n    \n      ENS Ulm\n      Enero 2022 - Julio 2022\n      Paris\n    \n    \n      Pasante en investigación en Inteligencia Artificial\n    \n  \n\n\nOtros\n\n  \n    \n      \n    \n    \n      Makers Fellowship (MF3)\n      Julio 2023 - Deciembre 2023\n    \n    \n      Business Fellow\n    \n  \n\n  \n\n  \n    \n      \n    \n    \n      LatinX in AI\n      Septiembre 2023 - Diciembre 2023\n    \n    \n      Mentorías en IA\n    \n  \n\n  \n\n  \n    \n      \n    \n    \n      AEFE\n      Junio 2020\n    \n    \n      Beca completa para estudios unviersitarios en Francia"
  },
  {
    "objectID": "blog/posts/23-10-2025/23-10-2025.html",
    "href": "blog/posts/23-10-2025/23-10-2025.html",
    "title": "¿Cómo utilizar la IA para entender mejor a sus usuarios?",
    "section": "",
    "text": "En general, las aplicaciones que se vuelven populares tienden a tener algún tipo de personalización. Un ejemplo de esto es Tiktok : si los usuarios pasan tanto tiempo en la aplicación es porque el contenido se alinea con lo que cada usuario quiere ver. Para lograrlo estas plataformas utilizan sistemas de recomendación, un tipo de algoritmos de IA que aprende a predecir si el usuario va a estar interesado en un ítem o no. Sin embargo, uno de los desafíos de este enfoque es que no aportan información acerca del usuario. Muchas veces se hacen recomendaciones sin entender el por qué, y aunque esto no es crítico, entender la razón puede ser extremadamente útil. Hoy voy a hablar de cómo se pueden usar los LLMs para entender mejor a los usuarios para mostrar que el uso de los LLMs no se limita a crear asistentes IA.\nEn un post publicado por DoorDash (un equivalente de Rappi, PedidosYa o UberEats en Estados Unidos) explican el método que han estado explorando. La idea consiste en darle información acerca de las compras realizadas por el usuario a un LLM para que genere una descripción. Un usuario puede ser caracterizado por “preferencias por la comida asiática y pokés” por ejemplo. Tener una descripción textual de este tipo parece algo trivial, pero a la escala de estas empresas con millones de usuarios hacerlo a mano es literalmente imposible. Más allá de automatizar el proceso, los LLMs son la herramienta que lo hace posible visto el volumen de datos por procesar.\nLa descripción del usuario luego puede ser utilizada con diferentes fines. Por una parte, tener una descripción textual de los usuarios facilita la exploración de los datos. Los equipos de marketing pueden usarla para entender mejor algunos segmentos de clientes, algo que a veces no es fácil o rápido de lograr. Ahora es más accesible entender que un cierto grupo de clientes es fánatico de la comida picante por ejemplo, permitiendo crear campañas de marketing, promociones, etc. más adaptadas. Por otra parte, esta misma descripción puede ser utilizada para mostrar mensajes más relevantes como “Porque te gusta lo picante” en lugar de “Tacos cerca de tí”. Incluso se podría utilizar para personalizar descripciones de tiendas o ítems según los intereses del usuario.\nEn este caso, el uso de LLMs permitió explicar mejor las preferencias de los usuarios (algo que difícil de lograr con los sistemas de recomendación tradicionales) lo que a su vez permite mejorar la personalización dentro de la plataforma y crear campañas de marketing con mejor targeting.\nEste es un buen ejemplo de cómo la IA está no solo mejorando, sino habilitando capacidades totalmente nuevas para las empresas. También, más allá de lo que puede aportar a una empresa, hay una línea fina entre “mejorar experiencia del usuario” y la invasión de la privacidad así que creo que es bueno tener una idea de cómo funcionan estos sistemas para poder juzgarlos uno mismo.\nEl blog completo de Doordash está aquí.\nSixto :)"
  },
  {
    "objectID": "blog/posts/05-05-2025/05-05-2025.html",
    "href": "blog/posts/05-05-2025/05-05-2025.html",
    "title": "¿Qué es la inteligencia artificial? – de cero",
    "section": "",
    "text": "De manera informal, la Inteligencia Artificial (IA) corresponde a los sistemas capaces de realizar tareas que uno no se esperaría que una computadora pueda hacer. El ejemplo estelar es ChatGPT. Antes de su aparición, nadie se imaginaba que las computadoras pudieran escribir ensayos. Aunque las computadoras ya nos superaban en ciertas tareas como en el cálculo de multiplicaciones, redactar un ensayo es un tipo de tarea muy diferente. Mientras que para realizar un cálculo basta con repetir una serie de instrucciones básicas (las mismas que vimos en primaria para sumar, restar, dividir, etc.), no se logra escribir un ensayo siguiendo instrucciones exactas. De hecho, muchos problemas no se resuelven siguiendo un manual de instrucciones. Hasta ahora, estas han sido las cosas en las que los humanos hemos sido mejores que las computadoras. Por ejemplo, no hay un manual para ser creativo ya que ser creativo es justamente lo opuesto de seguir un manual de instrucciones. Entonces, ¿cómo podemos lograr que una computadora realice tareas que no se pueden lograr siguiendo un conjunto de reglas fijas? Esto es lo que resuelve la IA. En este artículo voy a hablar de qué es la inteligencia artificial y de cómo funciona.\nEscribiendo este artículo me di cuenta que nunca había escuchado una definición de IA con la que quedara satisfecho. La clásica de “sistemas que imitan la inteligencia humana” siempre me pareció extremedamente vaga y no aclaraba nada. Era la misma frustración de buscar en el diccionario la definición de “zafio” y leer “propio o característico de la persona zafia”. Del otro lado del espectro, las definiciones más formales o precisas se sentían demasiado técnicas. Entre tantos detalles, identificar la idea general se hacía difícil. Era el equivalente de describir un carro como “vehículo automotor terrestre de cuatro ruedas con motor de combustión interna y transmisión mecánica o automática” en lugar de “un modo de transporte”. A pesar de nunca haber buscado la definición exacta de “carro”, todos tenemos una idea bastante clara de lo que es. Y esta idea nos la formamos a partir de los carros que en algún momento vimos y dijimos “ah, eso es un carro”. Fue un proceso de inducción : a partir de casos concretos inferimos reglas generales. A mi parecer, este método aclara mucho mejor qué es la IA.\nLos carros autónomos son un buen punto de partida para definir la IA. Estos carros tienen que ser capaces de reaccionar frente a obstáculos nuevos (como un objeto que nunca han visto) pero también tienen que funcionar en entornos nuevos (como una avenida que recién se construyó). Y esta es una característica fundamental de los sistemas de inteligencia artificial : a pesar de no haber sido programados explícitamente para gestionar cada contexto posible, saben hacerlo. El carro que se maneja solo, a pesar de nunca haber sido programado explícitamente para reaccionar a un objeto nuevo o conducir en una calle que nunca ha visto, sabe hacerlo. De manera general, lo que distingue un sistema tradicional de un sistema que usa IA es su capacidad a continuar funcionando a pesar de enfrentarse a un contexto o entorno desconocido. El término de “inteligencia” en inteligencia artificial viene de esta capacidad de generalizar o extrapolar experiencias previas a escenarios nuevos.\nPara mí, la IA corresponde a los sistemas capaces de lidiar con casos para los cuales no han sido explícitamente programados de gestionar. Podemos ver este mismo patrón en otros ejemplos.\nSistemas de recomendación\nLos sistemas de recomendación de Youtube, TikTok e Instagram permiten que el nuevo contenido que se va subiendo sea sugerido a los usuarios correctos. Esto se traduce en el hecho que Instagram y Tiktok reconocen los nuevos “trends” que nos gustan y en que Youtube nos sugiere nuevas canciones o nuevos artistas que podrían gustarnos. Si estas plataformas no pudieran recomendar el contenido nuevo, eventualmente los usuarios se aburrirían de ver lo mismo y la abandonarían. Por ello, es crucial tener sistemas capaces de recomendar las nuevas fotos, los nuevos trends, los nuevos videos, las nuevas canciones, etc. especialmente en las plataformas donde hay contenido nuevo constantemente. Nadie pasaría tanto tiempo en Tiktok e Instagram si siguieran recomendando memes de 2020 sobre el COVID en lugar de los trends más recientes como “lo mejor nunca se sube”, “la focking vibra”, etc.\nDetección de fraudes\nLa IA también es utilizada por diferentes empresas como Stripe para detectar casos de fraude. Si estos sistemas no fueran capaces de alertar sobre nuevos actores fraudulentos no tendrían mucho valor, ya que el problema es que estos actores utilizan nuevas identidades falsas, con nuevos métodos, desde nuevos lugares, etc. El éxito de estos sistemas depende de su capacidad de reconocer casos sospechosos a pesar de no ser iguales que los anteriores.\nAsistentes de voz\nY la lista puede seguir, hay muchas maneras más. La utilidad de un asistente de voz depende de su capacidad de entender la intención del usuario, incluso si el usuario expresa su intención de una manera totalmente nueva. La IA es lo que permite que los asistentes de voz entiendan la intención del usuario independientemente de cómo el usuario formule lo que necesita.\nFiltros de spam\nLos filtros que tenemos en nuestros correos usan IA para redirigir correos no deseados a una carpeta de “spam”. Estos filtros clasifican correctamente la gran mayoría de mensajes de spam incluso si vienen de cuentas nuevas y si están redactados de nuevas maneras.\nAhora la siguiente pregunta, es ¿cómo creamos estos sistemas?\nEl objetivo de este documento no es dar una taxonomía exhaustiva de todos tipos de IA que existen y de como funciona cada uno, sino dar la idea general para que cualquier persona pueda entender.\nImaginemos que queremos construir un sistema de IA capaz de decidir si un correo es “spam” o no. Nuestro objetivo es que este sistema clasifique correctamente la mayor cantidad posible de correos. Queremos clasificar los correos que son spam como “spam” y los mensajes reales como “no spam” (no queremos que correos importantes lleguen accidentalmente a la carpeta incorrecta).\nPara crear nuestro sistema comenzamos con tres ejemplos :\nPara entender un sistema de IA es crucial entender tres elementos : el modelo, los parámetros y el algoritmo de entrenamiento.\nPodemos ver un modelo como una plantilla para tomar decisiones. Una plantilla tiene ciertos elementos fijos y otros que se pueden modificar. Los parámetros son los elementos del modelo que se pueden modificar. El algoritmo 1 de entrenamiento es un manual que describe cómo modificar los parámetros para aumentar el número de respuestas correctas que el modelo genera.\nTomemos el ejemplo de un modelo de árbol de decisión. Un árbol de decisión es como un cuestionario con preguntas sí/no, donde cada respuesta lleva a otra pregunta o a una conclusión (“spam” o “no spam”).\nAquí presento de manera simplificada cómo se vería un algoritmo de entrenamiento. A cada etapa nuestro objetivo es modificar nuestro modelo para aumentar el número de respuestas correctas.\nPrimera etapa del algoritmo: Ya que casi todos los correos de spam contienen la palabra “felicidades”, si vemos esta palabra lo clasificaremos como “spam”.\nflowchart TD\n    A[¿El correo contiene 'felicidades'?] --&gt;|Sí| B[SPAM]\n    A --&gt;|No| C[NO SPAM]\nEn este caso, vamos a clasificar el mensaje “Felicidades por tu cumpleaños” como “spam”, lo cual es incorrecto.\nSegunda etapa del algoritmo: Corregimos los errores de la parte anterior. Los correos que tienen solo “felicidades” y no “dinero” no suelen ser “spam”, mientras que los correos que tienen “felicidades” y “dinero” si suelen ser spam. Así definimos la siguiente regla : si tienen la palabra “felicidades” y “dinero” los clasificaremos como “spam”.\nflowchart TD\n    A[¿El correo contiene 'felicidades'?] --&gt;|Sí| B[¿Contiene 'dinero'?]\n    B --&gt;|Sí| C[SPAM]\n    B --&gt;|No| D[NO SPAM]\n    A --&gt;|No| E[NO SPAM]\nPero en ese caso el mensaje “Hola sobrino, felicidades por tu cumpleaños! Te mando un abrazo y un sobre con dinero” va a ser incorrectamente clasificado como “spam”.\nTercera etapa del algoritmo : Para corregir este último error, si encontramos las palabras “ganaste”, “dinero” y “felicidades” vamos a clasificar el mensaje como “spam”.\nflowchart TD\n    A[¿El correo contiene 'felicidades'?] --&gt;|Sí| B[¿Contiene 'dinero'?]\n    B --&gt;|Sí| C[¿Contiene 'ganaste'?]\n    C --&gt;|Sí| D[SPAM]\n    C --&gt;|No| E[NO SPAM]\n    B --&gt;|No| F[NO SPAM]\n    A --&gt;|No| G[NO SPAM]\nAhora, si recibimos un nuevo correo que diga “¡felicidades por tu graduación!” este será correctamente clasificado como “no spam” mientras que “Felicidades, ganaste dinero en nuestro sorteo mensual” será clasificado como “spam”.\nEsto muestra, cómo a partir de algunos ejemplos podemos crear un sistema capaz de identificar patrones y de aplicarlos a casos nunca antes vistos.\nEs importante insistir en que es un proceso iterativo, es decir que se hace en varias etapas. En cada etapa ajustamos los parámetros (en este caso, las palabras clave y combinaciones) para mejorar las respuestas del modelo. Otro detalle importante es que supimos que modificar porque teníamos de antemano ejemplos de respuestas correctas. Sin ellas, no hubiéramos tenido nada con que comparar y no hubiéramos sabido que parámetros cambiar para aumentar el número de respuestas correctas.\nEs de notar también que las dificultades “técnicas” para entender la IA se encuentran en la parte del entrenamiento. La primera dificultad está en la concepción de los algoritmos de entrenamiento: ¿cómo definimos una serie de etapas que nos ayuden a encontrar los buenos parámetros? La segunda dificultad es el lado práctico : ¿estos algoritmos van a funcionar en la práctica? Podríamos crear un algoritmo que funciona bien en teoría pero que en la práctica tarda años en ejecutarse. Un ejemplo sería probar todas las combinaciones posibles de palabras para detectar si un correo es spam o no.\nPara responder a las preguntas iniciales : la IA corresponde a los sistemas capaces de gestionar casos para los que no han sido explícitamente programados de gestionar. Para crear estos sistemas, se necesita un modelo y un algoritmo. Comparando los resultados generados por el modelo con las respuestas correctas, el algoritmo modifica progresivamente los parámetros para maximizar el número de respuestas correctas generadas.\nHice muchas simplificaciones y omití ciertos conceptos que no me parecen esenciales para entender la idea general de la IA. Sin embargo, para cualquier duda mi correo está en mi sitio web, y espero poco a poco ir agregando contenido acerca de las cosas que no mencioné aquí.\nSixto :)"
  },
  {
    "objectID": "blog/posts/05-05-2025/05-05-2025.html#footnotes",
    "href": "blog/posts/05-05-2025/05-05-2025.html#footnotes",
    "title": "¿Qué es la inteligencia artificial? – de cero",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUn algoritmo es una lista de instrucciones↩︎"
  }
]